---
title: "Commenting Museum - Snippet Detection"
output: 
  html_document: 
      toc: TRUE
      code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(urltools)

load(file = "../data/df_snippet_info.RData")
df_snippet_info <- df_snippit_info %>% distinct() %>% rename("snippet" = "snippit")

source("../config/graphic-config.R")

snippets_to_search_for <- read_csv("../data/helper/22-03-09-snipits-download.csv") %>% 
  select("system" = `Commenting system`, "snippet" = Snipit) %>% 
  filter(!is.na(system))

nr_of_snippets <- snippets_to_search_for %>% nrow()

```


## Which domains are present in the german news site dataset?

Nicht alle Seiten sind gleichermaßen interessant, manche sind irrtümlich in den Datensatz geraten. Welche davon sind wichtig? Welche können gleich herausgefiltert werden?

On the bottom left corner of the table you can see, that overall 310 domains where found. 

```{r }

df_snippet_info %>% 
  group_by(site) %>% 
  summarise(count = n()) %>% 
  mutate(count = round(count/nr_of_snippets, digits = 0)) %>% 
  arrange(desc(count)) %>% 
  DT::datatable(.)

```

## Missing values: which domains could not be recognized?

The table above says, that there are more than 700 sites with missing values. What do this URLs look like?


```{r, echo=FALSE}

df_snippet_info %>% 
  filter(is.na(site)) %>% 
  DT::datatable(.)

```


```{r}
## snippets per Year and Domain - not necessary to show right now

df_snippets_per_year_domain <- df_snippet_info %>% 
  mutate(
    year = year(crawl_date)
    ) %>%
  group_by(year, site) %>% 
  summarise(counted_snippets = sum(detected, na.rm=TRUE), counted_sites = n()) %>% 
  mutate(counted_sites = round(counted_sites/nr_of_snippets, digits = 0)) %>% 
  ungroup() %>% 
  arrange(year, site) 

# DT::datatable(df_snippets_per_year_domain, rownames = FALSE)


```

## Which sites do have snippets of commenting systems in their content?

Again, in the bottom left corner you can see how many new sites have commenting systems according to your snippet table.

```{r}

df_domains_snippets <- df_snippets_per_year_domain %>% 
  filter(counted_snippets > 0) %>% 
  group_by(site) %>% 
  summarise(sum_snippets = sum(counted_snippets)) %>% 
  arrange(desc(sum_snippets))

DT::datatable(df_domains_snippets, rownames = FALSE)

```

## Snippets timeline (just table)

!!! This is weird: why are there two different snippets present, at least at "welt"? I thought, the snippets are to understand as this-OR-that, not both? So, in reality there are much less "unique" snippet-sites as it looked at first glance.

Checked on the source code: in the code of the sites from welt.de, both snippets (`disqus_thread` and `disqus.com/embed.js`) can be found. 

```{r}

df_testing_snippets_year <- df_snippet_info %>% 
  ungroup() %>% 
  filter(site %in% df_domains_snippets$site) %>% #View()
  mutate(year = year(crawl_date)) %>% #View()
  group_by(year, site, snippet) %>% 
  summarise(counted_snippets = sum(detected, na.rm=TRUE)) %>% 
  ungroup() %>% 
  arrange(desc(counted_snippets)) #%>% View()

DT::datatable(df_testing_snippets_year, rownames = FALSE)

```

## Snippets Counting - just for checking

Same counts as in the sheet from Robert. But why? At this point I know, my parser broke ~90% progress of parsing the html-files. Maybe the machines we are working on have similar RAM capacity and his broke as well? Unlikely. Note to myself: don't forget to dig deaper.

Next question of course: how much do the snippets overlap?

```{r}

df_counted_snippets <- df_testing_snippets_year %>% 
  ungroup() %>% 
  group_by(snippet) %>% 
  summarise(counted_snippets = sum(counted_snippets)) %>% 
  arrange(desc(counted_snippets)) 

DT::datatable(df_counted_snippets, rownames = FALSE)

```



## Snippets overlapping

The following table shows the combination of snippets summarized. First on the id's of the sites, later based on their combination. There are still weird things, I have to check on. Some id's are present in the data on more than one day, one example I checked on has been on two following days. This could be possible if the process of archiving this site was done over midnight ... but also then there should only be one date saved. 

As you can see at the table: there is no site using only the disqus-js-tag all alone. This snippet is, based on the current knowledge of the data, redundant. But, this is an indicator to dig deaper into the topic. One possible hypothesis on that: maybe the finding of implementing the js-tag on a site tells you, that this outlet is using disqus without any adaption. And news outlets that don't call the disqus-js as a direct dependency made adjustments to their needs, so that the original js-file is hidden from other js-files. Just fantasizing on that. 

```{r}

df_testing_snippet_combinations <- df_snippet_info %>% 
  ungroup() %>% 
  filter(site %in% df_domains_snippets$site, detected == 1) %>% #View()
  mutate(year = year(crawl_date)) %>% #View()
  group_by(sha1) %>% 
  summarise(counted_snippets = sum(detected, na.rm=TRUE), found_snippets = paste0(snippet, collapse = ", ")) %>% 
  ungroup() %>% 
  arrange(desc(counted_snippets)) %>% #View()
  group_by(found_snippets) %>% 
  summarise(snippet_combinations = n()) %>% #View()
  arrange(desc(snippet_combinations))
  
DT::datatable(df_testing_snippet_combinations, rownames = FALSE)


```


## Snippets timeline (graphical)

What you see on the left side is every site from the german dataset where snippets has been found. The x-axis stands for the time. For every year a snippet was present in the data, a dot is printed. The color indicates which snippet was detected. 

This is just a draft. Colors are awful, structure need to be improved. Why the graphic is still in this state, read the next part.

```{r }

snippets_to_look_at <- df_counted_snippets %>% 
  filter(counted_snippets > 0) %>% 
  select(snippet) %>% 
  mutate(snippet_order = row_number())

df_testing_snippets_year %>% 
  filter(snippet %in% snippets_to_look_at$snippet, counted_snippets > 0) %>% #View()
  ggplot(., aes(x = year, y = site, color = snippet)) +
  # geom_point(position = "jitter") +
  geom_jitter(width = .25, height = 0)+
  # facet_wrap(~site, ncol = 1)+
  theme_b03_dot_timeline

```

## The problem with date

The dataset contains a column `crawl_date` that I used to print the dots along the years. But as I search for answers regarding the overlapping of snippets, I recognised another thing. 

I am still very fond of jetzt.de. I saw in the graphic above no data for 2021 is printed for jetzt.de. I went back to the raw-file and filtered it down to the rows from 2021 and this site. There was data, no empty dataset. jetzt.de has been scanned by the archive 185 times in 2021, but no snippets where detected. So I opened a few links from the table in my browser. The date of publication in the browser showed 2018, not 2021 as in the table from the internet archive. I tested a few more sites, all of them where way older than the `crawl_date`. 

Why is that a problem? As I talked to Luca about these findings, he said "No problem, the assumption is, that older news sites are being changed according to new commenting software". But that's wrong, at least for all products of Sueddeutsche Zeitung, which expands on Sueddeutsche Magazin and jetzt.de. Old sites never get updated. Old site grew old and at some point they could no longer be used. The current technical stack that has to be maintained is always to much. They won't touch the old already published stuff. Sometimes it may happen, that journalist would reuse information from an old site (very unusual!), but then to workflow would be a kind of migration (copying the old content in a new site and refresh the info). 

So how to fix that problem with the current data available? You need to write parser for all the sites you wish to investigate, to extract the real date of publication. 

Or you request new data from the archive with the earliest date of crawling a site and check again, if at least the year of publication and the year of crawling is the same.


## New snippet to be added for coral and the annoying thing of hidden commenting sections

Sorry, more insights I gained, that should be shared. As I tried to understand the missing values for jetzt.de in 2021, I recognized that they pivoted from disqus to coral as a commenting system. I thought this must be visible in the data from 2021 (this was before I learned the difference between `crawl_date` and `publication_date` I explained above). So I went on a current site of jetzt.de and looked on the source code. There I saw one of the snippets you defined for coral. But it is inside an iframe, this could be a problem. So I went up in the hierarchy of tags and tried `"coral_community"`, which is used for the id of a div that holds the iframe. But this is still not the end: The comment section on jetzt.de these days is only visible, if you first click on a button "Kommentare anzeigen" (show comments). To find out if there is a commenting system implemented on jetzt.de currently, you have to search for `class="comments__button"`. 

I would recommend to check on other news sites and if they are hiding their commenting section behind a second click, to be sure you find what you are looking for.
