---
title: "Commenting Museum - Snippet Detection"
output: 
  html_document: 
      toc: TRUE
      code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(urltools)
library(MetBrewer)
library(BAMMtools)

load(file = "../data/df_snippet_info.RData")
df_snippet_info <- df_snippit_info %>% distinct() %>% rename("snippet" = "snippit") %>% 
  mutate(year = year(crawl_date))

source("../config/graphic-config.R")

snippets_to_search_for <- read_csv("../data/helper/22-03-09-snipits-download.csv") %>% 
  select("system" = `Commenting system`, "snippet" = Snipit) %>% 
  filter(!is.na(system))

nr_of_snippets <- snippets_to_search_for %>% nrow()

```

## Which domains are present in the german news site dataset?

Nicht alle Seiten sind gleichermaßen interessant, manche sind irrtümlich in den Datensatz geraten. Welche davon sind wichtig? Welche können gleich herausgefiltert werden?

On the bottom left corner of the table you can see, that overall 310 domains where found.

```{r }

df_snippet_info %>% 
  group_by(site) %>% 
  summarise(count = n()) %>% 
  mutate(count = round(count/nr_of_snippets, digits = 0)) %>% 
  arrange(desc(count)) %>% 
  DT::datatable(.)

```

## Missing values: which domains could not be recognized?

The table above says, that there are more than 700 sites with missing values. What do this URLs look like?

```{r, echo=FALSE}

df_snippet_info %>% 
  filter(is.na(site)) %>% 
  DT::datatable(.)

```

```{r}
## snippets per Year and Domain - not necessary to show right now

df_snippets_per_year_domain <- df_snippet_info %>% 
  mutate(
    year = year(crawl_date)
    ) %>%
  group_by(year, site) %>% 
  summarise(counted_snippets = sum(detected, na.rm=TRUE), counted_sites = n()) %>% 
  mutate(counted_sites = round(counted_sites/nr_of_snippets, digits = 0)) %>% 
  ungroup() %>% 
  arrange(year, site) 

# DT::datatable(df_snippets_per_year_domain, rownames = FALSE)


```

## Which sites do have snippets of commenting systems in their content?

Again, in the bottom left corner you can see how many new sites have commenting systems according to your snippet table.

```{r}

df_domains_snippets <- df_snippets_per_year_domain %>% 
  filter(counted_snippets > 0) %>% 
  group_by(site) %>% 
  summarise(sum_snippets = sum(counted_snippets)) %>% 
  arrange(desc(sum_snippets))

DT::datatable(df_domains_snippets, rownames = FALSE)

```

## Snippets timeline (just table)

!!! This is weird: why are there two different snippets present, at least at "welt"? I thought, the snippets are to understand as this-OR-that, not both? So, in reality there are much less "unique" snippet-sites as it looked at first glance.

Checked on the source code: in the code of the sites from welt.de, both snippets (`disqus_thread` and `disqus.com/embed.js`) can be found.

```{r}

df_testing_snippets_year <- df_snippet_info %>% 
  ungroup() %>% 
  filter(site %in% df_domains_snippets$site) %>% #View()
  mutate(year = year(crawl_date)) %>% #View()
  group_by(year, site, snippet) %>% 
  summarise(counted_snippets = sum(detected, na.rm=TRUE)) %>% 
  ungroup() %>% 
  arrange(desc(counted_snippets)) #%>% View()

DT::datatable(df_testing_snippets_year, rownames = FALSE)

```

## Snippets Counting - just for checking

Same counts as in the sheet from Robert. But why? At this point I know, my parser broke \~90% progress of parsing the html-files. Maybe the machines we are working on have similar RAM capacity and his broke as well? Unlikely. Note to myself: don't forget to dig deaper.

Next question of course: how much do the snippets overlap?

```{r}

df_counted_snippets <- df_testing_snippets_year %>% 
  ungroup() %>% 
  group_by(snippet) %>% 
  summarise(counted_snippets = sum(counted_snippets)) %>% 
  arrange(desc(counted_snippets)) 

DT::datatable(df_counted_snippets, rownames = FALSE)

```

## Snippets overlapping

The following table shows the combination of snippets summarized. First on the id's of the sites, later based on their combination. There are still weird things, I have to check on. Some id's are present in the data on more than one day, one example I checked on has been on two following days. This could be possible if the process of archiving this site was done over midnight ... but also then there should only be one date saved.

As you can see at the table: there is no site using only the disqus-js-tag all alone. This snippet is, based on the current knowledge of the data, redundant. But, this is an indicator to dig deaper into the topic. One possible hypothesis on that: maybe the finding of implementing the js-tag on a site tells you, that this outlet is using disqus without any adaption. And news outlets that don't call the disqus-js as a direct dependency made adjustments to their needs, so that the original js-file is hidden from other js-files. Just fantasizing on that.

```{r}

df_testing_snippet_combinations <- df_snippet_info %>% 
  ungroup() %>% 
  filter(site %in% df_domains_snippets$site, detected == 1) %>% #View()
  mutate(year = year(crawl_date)) %>% #View()
  group_by(sha1) %>% 
  summarise(counted_snippets = sum(detected, na.rm=TRUE), found_snippets = paste0(snippet, collapse = ", ")) %>% 
  ungroup() %>% 
  arrange(desc(counted_snippets)) %>% #View()
  group_by(found_snippets) %>% 
  summarise(snippet_combinations = n()) %>% #View()
  arrange(desc(snippet_combinations))
  
DT::datatable(df_testing_snippet_combinations, rownames = FALSE)


```

## Snippets timeline (graphical)

What you see on the left side is every site from the german dataset where snippets has been found. The x-axis stands for the time. For every year a snippet was present in the data, a dot is printed. The color indicates which snippet was detected.

This is just a draft. Colors are awful, structure need to be improved. Why the graphic is still in this state, read the next part.

```{r fig.height=4}

snippets_to_look_at <- df_counted_snippets %>% 
  filter(counted_snippets > 0) %>% 
  select(snippet) %>% 
  mutate(snippet_order = row_number())

year_breaks <- df_testing_snippets_year %>% select(year) %>% distinct() %>% pull(.)

df_snippets_year <- df_testing_snippets_year %>% 
  filter(snippet %in% snippets_to_look_at$snippet, counted_snippets > 0) #%>% #View()

save(df_snippets_year, file = "../shiny/b03-commenting-museum/data/df_snippets_year.RData")

df_snippits_year %>% 
  ggplot(., aes(x = year, y = site, color = snippet)) +
  # geom_point(position = "jitter") +
  geom_jitter(width = .2, height = 0)+
  # facet_wrap(~site, ncol = 1)+
  scale_color_manual(values = met.brewer("Troy", 6))+
  scale_x_continuous(breaks = year_breaks, name = "crawl_year") +
  theme_b03_dot_timeline

```



```{r fig.height=4}

df_counted_sites_year <- df_snippet_info %>% 
  filter(site %in% df_domains_snippets$site) %>% 
  select(year, site, sha1) %>% 
  distinct() %>% 
  group_by(year, site) %>% 
  summarise(counted_sites = n()) %>% 
  ungroup()

jenks_color_breaks <- getJenksBreaks(df_counted_sites_year %>% select(counted_sites) %>% pull(.), 5)

df_sites_year <- df_counted_sites_year %>% 
  pivot_wider(., id_cols = site, names_from = year, values_from = counted_sites) %>% #View()
  pivot_longer(., 2:last_col(), names_to = "year", values_to = "counted_sites") %>% 
  mutate(color_breaks = cut(counted_sites, 
                            breaks = jenks_color_breaks,
                            include.lowest = TRUE,
                            right = TRUE,
                            ordered_result = FALSE),
         year = as.numeric(year)
         ) #%>% 
save(df_sites_year, file = "../shiny/b03-commenting-museum/data/df_sites_year.RData")

df_graphic %>% 
  ggplot(aes(x = year, y = site, fill = color_breaks)) +
  geom_tile() +
  # scale_fill_met_d() +
  scale_fill_manual(values = met.brewer("Hokusai2"), na.value = "grey90") +
  scale_x_continuous(breaks = year_breaks, expand = c(0, NA), name = "crawl_year") +
  scale_y_discrete(expand = c(0, NA))
  
  
```




```{r fig.height=8}

df_graphic_month <- df_snippet_info %>% 
  filter(site %in% df_domains_snippets$site) %>% 
  mutate(month = month(crawl_date),
         fake_date = paste(year, month, "01", sep = "-") %>% as_date(.)
         ) %>% 
  select(year, month, fake_date, site, sha1) %>% 
  distinct() %>% #View()
  group_by(fake_date) %>% 
  summarise(counted_sites = n()) #%>% 
 
df_graphic_month %>%  ggplot(aes(x = fake_date, y = counted_sites)) +
  geom_bar(stat = "identity") +
  scale_x_date(date_breaks = "1 year", expand = c(0, NA), date_labels = "%Y", name = "crawl_year") 
  # facet_wrap(~site, ncol = 3)


```

